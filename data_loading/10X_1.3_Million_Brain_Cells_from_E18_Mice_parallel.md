Load in parallel 10X 1.3 Million Mouse Brain Cells
==================================================

The following steps will transfer the source data from https://support.10xgenomics.com/single-cell-gene-expression/datasets/1.3.0/1M_neurons
to Cloud Platform and load it into BigQuery. All of these steps occur in the
cloud instead of a local machine to reduce network data transfer time but some
of the cloud steps can be invoked from the local machine. It uses [Cloud Shell](https://github.com/googlegenomics/dsub) with [dsub](https://github.com/googlegenomics/dsub) for batch computing.

# Transfer the data to Google Cloud Platform.

1. Log into the [Google Cloud Shell](https://cloud.google.com/shell/docs/).
2. Transfer the data from 10X to your Cloud Shell instance. This takes ~2 minutes.
```
wget \
  http://cf.10xgenomics.com/samples/cell-exp/1.3.0/1M_neurons/1M_neurons_filtered_gene_bc_matrices_h5.h5 \
  -O /var/tmp/1M_neurons_filtered_gene_bc_matrices_h5.h5
```
3. Transfer the data from your Cloud Shell instance to your bucket:
```
gsutil cp /var/tmp/1M_neurons_filtered_gene_bc_matrices_h5.h5 gs://BUCKET-NAME
```

# Configure the local machine or Cloud Shell.

The rest of the steps can be run from either your local machine or the Cloud Shell
because they are just invoking processing in the cloud.

1. Obtain the code in this repository.
```
git clone https://github.com/broadinstitute/scalable_analytics.git
```
2. Install [dsub](https://github.com/googlegenomics/dsub).
```
pip install git+git://github.com/googlegenomics/dsub.git@master
```

# Build a Docker container for use with dsub.

Run this [Cloud Builder](https://cloud.google.com/container-builder/docs/) command
to build the Docker container in the cloud and store it in [Container Registry](https://cloud.google.com/container-registry/docs/). The Docker container
holds the Python dependencies and the Python script.

```
gcloud container builds submit \
  --tag gcr.io/PROJECT-ID/python2_hdf5 \
  scalable_analytics/data_loading/
```

# Run a dsub pipeline to reshape the data in parallel.

1. Edit [hdf5_to_sparse_tasks.tsv](./hdf5_to_sparse_tasks.tsv) replacing
'BUCKET-NAME' with your bucket name.
2. Run this dsub command to reshape in parallel sub matrices from the full HDF5
matrix. The parallelism of this operation is controlled by
[hdf5_to_sparse_tasks.tsv](./hdf5_to_sparse_tasks.tsv). This takes ~20 minutes.
```
dsub \
  --project PROJECT-ID \
  --zones "us-central1-*" \
  --logging gs://BUCKET-NAME/logs \
  --min-ram 32 \
  --image gcr.io/PROJECT-ID/python2_hdf5 \
  --tasks scalable_analytics/data_loading/hdf5_to_sparse_tasks.tsv \
  --command 'python /opt/hdf5_to_sparse.py' \
  --wait
```

# Load the data into BigQuery.

1. Create a destination BigQuery dataset either via the BigQuery Web UI or via
the [bq Command-Line Tool](https://cloud.google.com/bigquery/bq-command-line-tool).
```
bq mk DATASET-NAME
```
2. Load the data. This takes ~2 minutes.
```
bq load --autodetect DATASET-NAME.TABLE-NAME \
  gs://BUCKET-NAME/1M_neurons_filtered_gene_bc_matrices*.csv
```
