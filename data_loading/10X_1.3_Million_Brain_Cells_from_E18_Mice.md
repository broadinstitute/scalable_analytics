Load 10X 1.3 Million Mouse Brain Cells
======================================

The following steps will transfer the source data from https://support.10xgenomics.com/single-cell-gene-expression/datasets/1.3.0/1M_neurons
to Cloud Platform and load it into BigQuery. All of these steps occur in the
cloud instead of a local machine to reduce network data transfer time. It uses a Compute Engine instance with the default VM image to run a Python script.

# Create and configure a Compute Engine instance.

1. Use the [Cloud Console](https://console.cloud.google.com) to create and start a Compute Engine instance. For more detailed instructions please see the [Compute Engine documentation](https://cloud.google.com/compute/docs/instances/create-start-instance). Ensure that the instance:
    * has at least 32 GB of memory
    * resides in the same region as the destination Cloud Storage bucket
    * has "Allow full access to all Cloud APIs" checked
2. Use the Cloud Console to ssh to the new instance.
3. Add the Python dependencies:
```
sudo apt-get update
sudo apt-get install -y git python-setuptools
sudo easy_install pip
sudo pip install numpy scipy tables
```
4. Obtain the code in this repository.
```
git clone https://github.com/broadinstitute/scalable_analytics.git
```

# Transfer the data to Google Cloud Platform.

Transfer the data from 10X to your instance. This takes ~2 minutes.
```
wget \
  http://cf.10xgenomics.com/samples/cell-exp/1.3.0/1M_neurons/1M_neurons_filtered_gene_bc_matrices_h5.h5 \
  -O 1M_neurons_filtered_gene_bc_matrices_h5.h5
```

# Reshape the data.

Run the script convert the data from HDF5 format to a tidy data CSV. This takes
~3 hours for the entire 4 GB HDF5 file when run on Compute Engine utilizing
[streaming upload to Cloud Storage](https://cloud.google.com/storage/docs/gsutil/commands/cp#streaming-transfers).

```
python scalable_analytics/data_loading/hdf5_to_sparse.py \
  | gsutil cp - gs://BUCKET-NAME/1M_neurons_filtered_gene_bc_matrices_h5.csv
```

# Load the data into BigQuery.

1. Create a destination BigQuery dataset either via the BigQuery Web UI or via
the [bq Command-Line Tool](https://cloud.google.com/bigquery/bq-command-line-tool).
```
bq mk DATASET-NAME
```
2. Load the data. This takes ~2 minutes.
```
bq load --autodetect DATASET-NAME.TABLE-NAME \
  gs://BUCKET-NAME/1M_neurons_filtered_gene_bc_matrices_h5.csv
```
